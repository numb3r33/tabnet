{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/tabnet\n",
      "\u001b[K     |████████████████████████████████| 46 kB 4.7 MB/s \n",
      "\u001b[K     |████████████████████████████████| 56 kB 4.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 51 kB 370 kB/s \n",
      "\u001b[K     |████████████████████████████████| 186 kB 27.9 MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "path_to_repository = 'tabnet'\n",
    "library_name       = 'tabnet'\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd 'drive/MyDrive/{path_to_repository}'\n",
    "!pip3 install -e . -q\n",
    "!pip3 install nbdev -q\n",
    "!pip3 install fastai==2.5.2 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabNet\n",
    "\n",
    "> Implementation of tabnet paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import scipy as sp\n",
    "from fastai.tabular.all import *\n",
    "from torch.autograd     import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_emb_sz(to, sz_dict=None):\n",
    "    \"Get default embedding size from `TabularPreprocessor` `proc` or the ones in `sz_dict`\"\n",
    "    return [_one_emb_sz(to.classes, n, sz_dict) for n in to.cat_names]\n",
    "\n",
    "def _one_emb_sz(classes, n, sz_dict=None):\n",
    "    \"Pick an embedding size for `n` depending on `classes` if not given in `sz_dict`.\"\n",
    "    sz_dict = ifnone(sz_dict, {})\n",
    "    n_cat = len(classes[n])\n",
    "    sz = sz_dict.get(n, int(emb_sz_rule(n_cat)))  # rule of thumb\n",
    "    return n_cat,sz\n",
    "\n",
    "def emb_sz_rule(n_cat):\n",
    "    \"Rule of thumb to pick embedding size corresponding to `n_cat`\"\n",
    "    return min(600, round(1.6 * n_cat**0.56))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_explain_matrix(input_dim, cat_emb_dim, cat_idxs, post_embed_dim):\n",
    "    \"\"\"\n",
    "    This is a computational trick.\n",
    "    In order to rapidly sum importances from same embeddings\n",
    "    to the initial index.\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dim : int\n",
    "        Initial input dim\n",
    "    cat_emb_dim : int or list of int\n",
    "        if int : size of embedding for all categorical feature\n",
    "        if list of int : size of embedding for each categorical feature\n",
    "    cat_idxs : list of int\n",
    "        Initial position of categorical features\n",
    "    post_embed_dim : int\n",
    "        Post embedding inputs dimension\n",
    "    Returns\n",
    "    -------\n",
    "    reducing_matrix : np.array\n",
    "        Matrix of dim (post_embed_dim, input_dim)  to performe reduce\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(cat_emb_dim, int):\n",
    "        all_emb_impact = [cat_emb_dim - 1] * len(cat_idxs)\n",
    "    else:\n",
    "        all_emb_impact = [emb_dim - 1 for emb_dim in cat_emb_dim]\n",
    "\n",
    "    acc_emb = 0\n",
    "    nb_emb = 0\n",
    "    indices_trick = []\n",
    "    for i in range(input_dim):\n",
    "        if i not in cat_idxs:\n",
    "            indices_trick.append([i + acc_emb])\n",
    "        else:\n",
    "            indices_trick.append(\n",
    "                range(i + acc_emb, i + acc_emb + all_emb_impact[nb_emb] + 1)\n",
    "            )\n",
    "            acc_emb += all_emb_impact[nb_emb]\n",
    "            nb_emb += 1\n",
    "\n",
    "    reducing_matrix = np.zeros((post_embed_dim, input_dim))\n",
    "    for i, cols in enumerate(indices_trick):\n",
    "        reducing_matrix[cols, i] = 1\n",
    "\n",
    "    return sp.sparse.csc_matrix(reducing_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def make_dl():\n",
    "  path = untar_data(URLs.ADULT_SAMPLE)\n",
    "  df = pd.read_csv(path / 'adult.csv')\n",
    "  cat_names  = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\n",
    "  cont_names = ['age', 'fnlwgt', 'education-num']\n",
    "\n",
    "  np.random.seed(41)\n",
    "  splits = RandomSplitter()(range_of(df))\n",
    "\n",
    "  cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\n",
    "  cont_names = ['age', 'fnlwgt', 'education-num']\n",
    "  procs = [Categorify, FillMissing, Normalize]\n",
    "  y_names = 'salary'\n",
    "  y_block = CategoryBlock()\n",
    "\n",
    "  to = TabularPandas(df, procs=procs, cat_names=cat_names, cont_names=cont_names,\n",
    "                    y_names=y_names, y_block=y_block, splits=splits)\n",
    "  \n",
    "  return to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "to  = make_dl()\n",
    "dls = to.dataloaders(bs=4096)\n",
    "emb_szs = get_emb_sz(to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "\n",
    "### Main components of the paper\n",
    "\n",
    "  - TabNetEncoder\n",
    "    - Feature Transformer\n",
    "      - Primitive Block\n",
    "        - FC + Ghost BN + GLU\n",
    "    - Attention Transformer\n",
    "      - FC, BN, Priors and SparseMax\n",
    "    - Split\n",
    "\n",
    "  - TabNetDecoder\n",
    "\n",
    "### How to create a primitive block?\n",
    "  - Primitive Block consists of FC ( fully-connected ) layer, ghost batch norm and GLU ( Gated Linear Unit ).\n",
    "  - According to design mentioned in the paper we have to support construction of multiple of these blocks.\n",
    "\n",
    "### What is the notion of decision step?\n",
    "  - Decision step is when we run tabnet encoder multiple times.\n",
    "\n",
    "### What is a shared decision unit?\n",
    "  - During multiple runs of the tabnet encoder the section of the feature transformer that would share weights across the runs.\n",
    "\n",
    "### What is an independent decision unit?\n",
    "  - During multiple runs of the tabnet encoder the section of the feature transformer that would not share weights across the runs.\n",
    "\n",
    "### Which components of the primitive blocks are parameterized?\n",
    "  - FC\n",
    "  - Ghost BN ( batch_size, momentum )\n",
    "\n",
    "### What is meant by muliple copies of the primitive block?\n",
    "  - So the paper proposes to try out building multiple different copies of the primitive block, every copy would require us to initialize different parameters which could or could not be shared across different decision steps.\n",
    "\n",
    "### Notes:\n",
    "  - In this architecture we are assuming that shared layer would always come before the independent decision step layer, but what would happend there is no shared layer and we only want independent decision layer.\n",
    "  - Also in feature transformer, Linear layer takes input_dim to be #features and output dimension would be 2 * (n_d + n_a) where n_d represents `dimension of the prediction layer` and n_a represents `dimension of the attention layer`.\n",
    "\n",
    "### Todos:\n",
    "  - [x] initialize glu layer particular linear layer with xavier initializaion, look at fastai docs to see how linear layer is initialized in the module.\n",
    "  - [x] Attention Module\n",
    "  - [x] Split Module\n",
    "  - [x] Loss function\n",
    "  - [x] Introduce $\\lambda_{sparse}$ and add it to the overall loss.\n",
    "  - [ ] TabNetDecoder ( for self-supervised learning )\n",
    "  - [ ] Explainable AI through global and instance level feature importance.\n",
    "  - [ ] How to fit decoder network in the cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def init_glu_linear(module, input_dim, output_dim):\n",
    "    gain_value = np.sqrt((input_dim + output_dim) / np.sqrt(4 * input_dim))\n",
    "    nn.init.xavier_normal_(module.weight, gain=gain_value)\n",
    "\n",
    "def init_non_glu_linear(module, input_dim, output_dim):\n",
    "    gain_value = np.sqrt((input_dim + output_dim) / np.sqrt(input_dim))\n",
    "    torch.nn.init.xavier_normal_(module.weight, gain=gain_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainable Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TNEMbedding(Module):\n",
    "  \"Embedding layer used in Tab Net\"\n",
    "  def __init__(self, emb_szs, n_cont):\n",
    "      self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])\n",
    "      n_emb = sum(e.embedding_dim for e in self.embeds)\n",
    "      self.n_emb,self.n_cont = n_emb,n_cont\n",
    "      sizes = [n_emb + n_cont]\n",
    "      \n",
    "  def forward(self, x_cat, x_cont=None):\n",
    "      \"\"\"\n",
    "      Runs through categorical features and transform them into\n",
    "      embeding based on the `emb_szs` passed in the layer constructor.\n",
    "      It then concatenates the remaining continuous features if any with\n",
    "      the embeddings.\n",
    "      \"\"\"\n",
    "\n",
    "      if self.n_emb != 0:\n",
    "          x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n",
    "          x = torch.cat(x, 1)\n",
    "      \n",
    "      if self.n_cont != 0:\n",
    "          x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n",
    "      \n",
    "      return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ghost Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GBN(Module):\n",
    "    \"\"\"\n",
    "    Ghost Batch Normalization\n",
    "    https://arxiv.org/abs/1705.08741\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inp_dim, vbs=128, mom=0.01):\n",
    "        store_attr()\n",
    "        self.bn = nn.BatchNorm1d(self.inp_dim, momentum=self.mom)\n",
    "\n",
    "    def forward(self, x):\n",
    "        chunks = x.chunk(int(np.ceil(x.shape[0] / self.vbs)), 0)\n",
    "        res    = [self.bn(x_) for x_ in chunks]\n",
    "        return torch.cat(res, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLU Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GLUBlock(Module):\n",
    "  def __init__(self, inp_dim, out_dim, fc=None, vbs=128, mom=0.02):\n",
    "    store_attr()\n",
    "\n",
    "    self.fc = ifnone(fc, nn.Linear(inp_dim, 2 * out_dim, bias=False))\n",
    "    init_glu_linear(self.fc, inp_dim, 2 * out_dim)\n",
    "    \n",
    "    self.bn = GBN(2 * out_dim, vbs=vbs, mom=mom)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.fc(x)\n",
    "    out = self.bn(out)\n",
    "    out = torch.mul(out[:, : self.out_dim], torch.sigmoid(out[:, self.out_dim :]))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FeatureTransformer(Module):\n",
    "  def __init__(self, inp_dim, out_dim, n_d, n_a, n_sh, n_ind, vbs, mom):\n",
    "    store_attr()\n",
    "\n",
    "    self.fcs    = self._make_fc(n_sh)\n",
    "    self.sh_ds  = self._make_shared_blocks()\n",
    "    self.ind_ds = self._make_ind_blocks() \n",
    "\n",
    "  \n",
    "  def _make_fc(self, n):\n",
    "    return nn.ModuleList([nn.Linear(self.inp_dim, \n",
    "                                    2 * (self.n_d + self.n_a), \n",
    "                                    bias=False) if i == 0 else \\\n",
    "                          nn.Linear(self.n_d + self.n_a, \n",
    "                                    2 * (self.n_d + self.n_a),\n",
    "                                    bias=False\n",
    "                                    ) for i in range(n)])\n",
    "\n",
    "  def _make_shared_blocks(self):\n",
    "    return nn.ModuleList([GLUBlock(self.inp_dim, \n",
    "                                    self.out_dim, \n",
    "                                    fc=self.fcs[i], \n",
    "                                    vbs=self.vbs, \n",
    "                                    mom=self.mom) if i == 0 else\\\n",
    "                           GLUBlock(self.out_dim, \n",
    "                                    self.out_dim, \n",
    "                                    fc=self.fcs[i], \n",
    "                                    vbs=self.vbs, \n",
    "                                    mom=self.mom) for i in range(self.n_sh)])\n",
    "    \n",
    "  def _make_ind_blocks(self):\n",
    "    return nn.ModuleList([GLUBlock(self.out_dim, \n",
    "                                   self.out_dim, \n",
    "                                   fc=None, \n",
    "                                   vbs=self.vbs, \n",
    "                                   mom=self.mom) for i in range(self.n_ind)]\n",
    "                         )\n",
    "\n",
    "  def forward(self, x):\n",
    "    scale = torch.sqrt(torch.FloatTensor([0.5])).to(x.device)\n",
    "\n",
    "    out = self.sh_ds[0](x)\n",
    "    \n",
    "    for i in range(1, len(self.sh_ds)):\n",
    "      out = torch.add(out, self.sh_ds[i](out))\n",
    "      out = out * scale\n",
    "\n",
    "    for i in range(len(self.ind_ds)):\n",
    "      out = torch.add(out, self.ind_ds[i](out))\n",
    "      out = out * scale\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparsemax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# credits to Yandex https://github.com/Qwicen/node/blob/master/lib/nn_utils.py\n",
    "def _make_ix_like(input, dim=0):\n",
    "    d = input.size(dim)\n",
    "    rho = torch.arange(1, d + 1, device=input.device, dtype=input.dtype)\n",
    "    view = [1] * input.dim()\n",
    "    view[0] = -1\n",
    "    return rho.view(view).transpose(0, dim)\n",
    "\n",
    "\n",
    "class SparsemaxFunction(Function):\n",
    "    \"\"\"\n",
    "    An implementation of sparsemax (Martins & Astudillo, 2016). See\n",
    "    :cite:`DBLP:journals/corr/MartinsA16` for detailed description.\n",
    "    By Ben Peters and Vlad Niculae\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, dim=-1):\n",
    "        \"\"\"sparsemax: normalizing sparse transform (a la softmax)\n",
    "        Parameters\n",
    "        ----------\n",
    "        ctx : torch.autograd.function._ContextMethodMixin\n",
    "        input : torch.Tensor\n",
    "            any shape\n",
    "        dim : int\n",
    "            dimension along which to apply sparsemax\n",
    "        Returns\n",
    "        -------\n",
    "        output : torch.Tensor\n",
    "            same shape as input\n",
    "        \"\"\"\n",
    "        ctx.dim = dim\n",
    "        max_val, _ = input.max(dim=dim, keepdim=True)\n",
    "        input -= max_val  # same numerical stability trick as for softmax\n",
    "        tau, supp_size = SparsemaxFunction._threshold_and_support(input, dim=dim)\n",
    "        output = torch.clamp(input - tau, min=0)\n",
    "        ctx.save_for_backward(supp_size, output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        supp_size, output = ctx.saved_tensors\n",
    "        dim = ctx.dim\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[output == 0] = 0\n",
    "\n",
    "        v_hat = grad_input.sum(dim=dim) / supp_size.to(output.dtype).squeeze()\n",
    "        v_hat = v_hat.unsqueeze(dim)\n",
    "        grad_input = torch.where(output != 0, grad_input - v_hat, grad_input)\n",
    "        return grad_input, None\n",
    "\n",
    "    @staticmethod\n",
    "    def _threshold_and_support(input, dim=-1):\n",
    "        \"\"\"Sparsemax building block: compute the threshold\n",
    "        Parameters\n",
    "        ----------\n",
    "        input: torch.Tensor\n",
    "            any dimension\n",
    "        dim : int\n",
    "            dimension along which to apply the sparsemax\n",
    "        Returns\n",
    "        -------\n",
    "        tau : torch.Tensor\n",
    "            the threshold value\n",
    "        support_size : torch.Tensor\n",
    "        \"\"\"\n",
    "\n",
    "        input_srt, _ = torch.sort(input, descending=True, dim=dim)\n",
    "        input_cumsum = input_srt.cumsum(dim) - 1\n",
    "        rhos = _make_ix_like(input, dim)\n",
    "        support = rhos * input_srt > input_cumsum\n",
    "\n",
    "        support_size = support.sum(dim=dim).unsqueeze(dim)\n",
    "        tau = input_cumsum.gather(dim, support_size - 1)\n",
    "        tau /= support_size.to(input.dtype)\n",
    "        return tau, support_size\n",
    "\n",
    "\n",
    "sparsemax = SparsemaxFunction.apply\n",
    "\n",
    "class Sparsemax(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        self.dim = dim\n",
    "        super(Sparsemax, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return sparsemax(input, self.dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attentive Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AttentiveTransformer(Module):\n",
    "  def __init__(self, inp_dim, out_dim, vbs, mom, mask_type):\n",
    "    store_attr()\n",
    "    self.fc = nn.Linear(inp_dim, out_dim, bias=False)\n",
    "    init_non_glu_linear(self.fc, inp_dim, out_dim)\n",
    "    \n",
    "    self.bn = GBN(out_dim, vbs=vbs, mom=mom)\n",
    "\n",
    "    mask_type = ifnone(mask_type, 'sparsemax')\n",
    "    if mask_type == 'sparsemax': self.sel = Sparsemax(dim=-1)\n",
    "\n",
    "  def forward(self, priors, proc_feat):\n",
    "    out = self.fc(proc_feat)\n",
    "    out = self.bn(out)\n",
    "    out = torch.mul(out, priors)\n",
    "    out = self.sel(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TabNetEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TabNetEncoder(Module):\n",
    "  def __init__(self, \n",
    "               inp_dim, \n",
    "               out_dim,\n",
    "               n_d,\n",
    "               n_a,\n",
    "               n_ds,\n",
    "               gamma=1.3,\n",
    "               n_ind=2,\n",
    "               n_sh=2,\n",
    "               eps=1e-15,\n",
    "               vbs=128,\n",
    "               mom=0.02,\n",
    "               mask_type='sparsemax'\n",
    "               ):\n",
    "    \n",
    "    store_attr()\n",
    "    self.init_bn = nn.BatchNorm1d(self.inp_dim, momentum=0.01)\n",
    "    self.init_ft = FeatureTransformer(inp_dim,\n",
    "                                      n_d + n_a,\n",
    "                                      n_d,\n",
    "                                      n_a,\n",
    "                                      n_sh, \n",
    "                                      n_ind, \n",
    "                                      vbs, \n",
    "                                      mom\n",
    "                                      )\n",
    "    \n",
    "    # based on number of decision steps we would create module\n",
    "    # list of transformers and attentive transformers\n",
    "    self.fts = nn.ModuleList()\n",
    "    self.ats = nn.ModuleList()\n",
    "\n",
    "    for i in range(n_ds):\n",
    "      ft = FeatureTransformer(inp_dim,\n",
    "                              n_d + n_a,\n",
    "                              n_d,\n",
    "                              n_a,\n",
    "                              n_sh, \n",
    "                              n_ind, \n",
    "                              vbs, \n",
    "                              mom\n",
    "                              )\n",
    "      \n",
    "      # attentive transformer is always preceded by \n",
    "      # feature transformer hence inp_dim would be \n",
    "      # `n_d + n_a`\n",
    "      \n",
    "      at = AttentiveTransformer(n_a,\n",
    "                                inp_dim,\n",
    "                                vbs=vbs,\n",
    "                                mom=mom,\n",
    "                                mask_type=mask_type\n",
    "                                )\n",
    "      \n",
    "      self.fts.append(ft)\n",
    "      self.ats.append(at)\n",
    "\n",
    "  def forward(self, x, priors=None):\n",
    "    x = self.init_bn(x)\n",
    "\n",
    "    priors = ifnone(priors, torch.ones(x.shape).to(x.device))\n",
    "\n",
    "    M_loss = 0\n",
    "    att    = self.init_ft(x)[:, self.n_d:] # this could be configured\n",
    "\n",
    "    steps_out = []\n",
    "\n",
    "    for step in range(self.n_ds):\n",
    "      M = self.ats[step](priors, att)\n",
    "      M_loss += torch.mean(torch.sum(torch.mul(M, torch.log(M + self.eps)), dim=1)\n",
    "                          )\n",
    "      \n",
    "      # update prior\n",
    "      priors = torch.mul(self.gamma - priors, priors)\n",
    "\n",
    "      # output\n",
    "      masked_x = torch.mul(M, x)\n",
    "      out      = self.fts[step](masked_x)\n",
    "      d        = nn.ReLU()(out[:, :self.n_d])\n",
    "\n",
    "      steps_out.append(d)\n",
    "\n",
    "      # update attention\n",
    "      att = out[:, self.n_d:]\n",
    "\n",
    "    M_loss /= self.n_ds\n",
    "    return steps_out, M_loss\n",
    "\n",
    "  def forward_masks(self, x):\n",
    "    x = self.init_bn(x)\n",
    "    priors = torch.ones(x.shape).to(x.device)\n",
    "    M_explain = torch.zeros(x.shape).to(x.device)\n",
    "    att = self.init_ft(x)[:, self.n_d:]\n",
    "    masks = {}\n",
    "\n",
    "    for step in range(self.n_ds):\n",
    "      M = self.ats[step](priors, att)\n",
    "      masks[step] = M\n",
    "\n",
    "      # update priors\n",
    "      priors = torch.mul(self.gamma - priors, priors)\n",
    "      # output\n",
    "      masked_x = torch.mul(M, x)\n",
    "      out = self.fts[step](masked_x)\n",
    "      d   = nn.ReLU()(out[:, :self.n_d])\n",
    "\n",
    "      # explain aggregation\n",
    "      step_importance = torch.sum(d, axis=1)\n",
    "      M_explain += torch.mul(M, step_importance.unsqueeze(dim=1))\n",
    "\n",
    "      # update attention\n",
    "      att = out[:, self.n_d:]\n",
    "    \n",
    "    return M_explain, masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TabNet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TabNet(Module):\n",
    "  def __init__(self,\n",
    "               emb_szs, \n",
    "               n_cont,\n",
    "               out_dim,\n",
    "               n_d=2,\n",
    "               n_a=2,\n",
    "               n_ds=2,\n",
    "               gamma=1.3,\n",
    "               n_ind=2,\n",
    "               n_sh=2,\n",
    "               eps=1e-15,\n",
    "               vbs=128,\n",
    "               mom=0.02,\n",
    "               mask_type=\"sparsemax\",\n",
    "               ):\n",
    "    \n",
    "    store_attr()\n",
    "    self.tnembed = TNEMbedding(emb_szs, n_cont)\n",
    "    self.encoder = TabNetEncoder(inp_dim=self.tnembed.n_emb + n_cont, \n",
    "                                out_dim=out_dim,\n",
    "                                n_d=n_d,\n",
    "                                n_a=n_a,\n",
    "                                n_ds=n_ds,\n",
    "                                gamma=gamma,\n",
    "                                n_ind=n_ind,\n",
    "                                n_sh=n_sh,\n",
    "                                eps=eps,\n",
    "                                vbs=vbs,\n",
    "                                mom=mom,\n",
    "                                mask_type=mask_type\n",
    "                                )\n",
    "    \n",
    "    self.final_mapping = nn.Linear(n_d, out_dim, bias=False)\n",
    "    init_non_glu_linear(self.final_mapping, n_d, out_dim)\n",
    "\n",
    "  def forward(self, x_cat, x_cont, att=False):\n",
    "    x  = self.tnembed(x_cat, x_cont)\n",
    "\n",
    "    res = 0\n",
    "    steps_output, M_loss = self.encoder(x)\n",
    "    res = torch.sum(torch.stack(steps_output, dim=0), dim=0)\n",
    "    out = self.final_mapping(res)\n",
    "\n",
    "    if att:\n",
    "      M_explain, masks = self.encoder.forward_masks(x)\n",
    "      return out, M_loss, M_explain, masks\n",
    "    \n",
    "    return out, M_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TabNetLoss(Module):\n",
    "  def __init__(self, lambda_sparse:float, axis:int=-1):\n",
    "    store_attr()\n",
    "    self.loss_fn = CrossEntropyLossFlat()\n",
    "  \n",
    "  def forward(self, os, target):\n",
    "      output, M_loss = os\n",
    "      tot            = self.loss_fn(output, target) +\\\n",
    "                       M_loss * self.lambda_sparse\n",
    "      return tot\n",
    "  def decodes(self, x):    return x.argmax(dim=self.axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "model = TabNet(emb_szs=get_emb_sz(to),\n",
    "             n_cont=len(to.cont_names), \n",
    "             out_dim=dls.c, \n",
    "             n_d=16, \n",
    "             n_a=16,\n",
    "             n_ds=5, \n",
    "             n_sh=2,\n",
    "             n_ind=2, \n",
    "             gamma=1.5,\n",
    "             vbs=128, \n",
    "             mom=0.02\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def custom_accuracy(inps, targ, axis=-1):\n",
    "  \"Compute accuracy with `targ` when `pred` is bs * n_classes\"\n",
    "  inp, _ = inps\n",
    "  pred,targ = flatten_check(inp.argmax(dim=axis), targ)\n",
    "  return (pred == targ).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "lambda_sparse = 1e-4\n",
    "learn = Learner(dls, model, TabNetLoss(lambda_sparse), opt_func=ranger, metrics=[custom_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #hide\n",
    "# lambda_sparse = 1e-4\n",
    "# opt_func = partial(Adam, wd=0.01, eps=1e-5)\n",
    "# learn = Learner(dls, model, TabNetLoss(lambda_sparse), opt_func=opt_func, lr=1e-2, metrics=[custom_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>custom_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.421572</td>\n",
       "      <td>0.426996</td>\n",
       "      <td>0.799447</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.415917</td>\n",
       "      <td>0.412212</td>\n",
       "      <td>0.804975</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.408392</td>\n",
       "      <td>0.400925</td>\n",
       "      <td>0.808968</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.402422</td>\n",
       "      <td>0.384625</td>\n",
       "      <td>0.820332</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.397110</td>\n",
       "      <td>0.379603</td>\n",
       "      <td>0.822021</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "learn.fit_flat_cos(5, lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "path   = untar_data(URLs.ADULT_SAMPLE)\n",
    "df     = pd.read_csv(path / 'adult.csv')\n",
    "tmp_dl = learn.dls.test_dl(df.iloc[:20], bs=1)\n",
    "batch  = tmp_dl.one_batch()\n",
    "\n",
    "cat_dims = [emb_szs[i][1] for i in range(len(emb_szs))]\n",
    "cat_idxs = [3,4,5,6,7,8, 9]\n",
    "tot = len(to.cont_names) + len(to.cat_names)\n",
    "\n",
    "matrix = create_explain_matrix(tot,\n",
    "                      cat_dims,\n",
    "                      cat_idxs,\n",
    "                      42)\n",
    "\n",
    "tmp_dl = learn.dls.test_dl(df.iloc[:20], bs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import pdb\n",
    "@patch\n",
    "def explain(x:Learner, dl:TabDataLoader):\n",
    "  #pdb.set_trace()\n",
    "  \"Get explain values for a set of predictions\"\n",
    "  dec_y = []\n",
    "  x.model.eval()\n",
    "  for batch_nb, data in enumerate(dl):\n",
    "    with torch.no_grad():\n",
    "      out, M_loss, M_explain, masks = x.model(data[0], data[1], True)\n",
    "    for key, value in masks.items():\n",
    "      masks[key] = sp.sparse.csc_matrix.dot(value.cpu().numpy(), matrix)\n",
    "    if batch_nb == 0:\n",
    "      res_explain = sp.sparse.csc_matrix.dot(M_explain.cpu().numpy(),\n",
    "                                  matrix)\n",
    "      res_masks = masks\n",
    "    else:\n",
    "      res_explain = np.vstack([res_explain,\n",
    "                              sp.sparse.csc_matrix.dot(M_explain.cpu().numpy(),\n",
    "                                              matrix)])\n",
    "      for key, value in masks.items():\n",
    "        res_masks[key] = np.vstack([res_masks[key], value])\n",
    "\n",
    "    dec_y.append(int(learn.loss_func.decodes(out)))\n",
    "  return dec_y, res_masks, res_explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "ndec_y, res_masks, res_explain = learn.explain(tmp_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def plot_explain(masks, lbls, figsize=(12,12)):\n",
    "  \"Plots masks with `lbls` (`dls.x_names`)\"\n",
    "  fig = plt.figure(figsize=figsize)\n",
    "  ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "  plt.yticks(np.arange(0, len(masks[0]), 1.0))\n",
    "  plt.xticks(np.arange(0, len(masks[0][0]), 1.0))\n",
    "  ax.set_xticklabels(lbls, rotation=90)\n",
    "  plt.ylabel('Sample Number')\n",
    "  plt.xlabel('Variable')\n",
    "  plt.imshow(masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "lbls = dls.x_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "plot_explain(res_masks, lbls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
