{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/tabnet\n",
      "\u001b[K     |████████████████████████████████| 46 kB 3.9 MB/s \n",
      "\u001b[K     |████████████████████████████████| 51 kB 267 kB/s \n",
      "\u001b[K     |████████████████████████████████| 56 kB 3.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 186 kB 31.4 MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "path_to_repository = 'tabnet'\n",
    "library_name       = 'tabnet'\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd 'drive/MyDrive/{path_to_repository}'\n",
    "!pip3 install -e . -q\n",
    "!pip3 install nbdev -q\n",
    "!pip3 install fastai==2.5.2 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabNet\n",
    "\n",
    "> Implementation of tabnet paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.tabular.all import *\n",
    "from torch.autograd     import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='974848' class='' max='968212' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.69% [974848/968212 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#export\n",
    "path = untar_data(URLs.ADULT_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>Private</td>\n",
       "      <td>101320</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>1902</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;=50k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>236746</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>10520</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;=50k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>96185</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;50k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>112847</td>\n",
       "      <td>Prof-school</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;=50k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>82297</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;50k</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt  ... hours-per-week  native-country salary\n",
       "0   49            Private  101320  ...             40   United-States  >=50k\n",
       "1   44            Private  236746  ...             45   United-States  >=50k\n",
       "2   38            Private   96185  ...             32   United-States   <50k\n",
       "3   38       Self-emp-inc  112847  ...             40   United-States  >=50k\n",
       "4   42   Self-emp-not-inc   82297  ...             50   United-States   <50k\n",
       "\n",
       "[5 rows x 15 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#export\n",
    "df = pd.read_csv(path / 'adult.csv')\n",
    "cat_names  = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\n",
    "cont_names = ['age', 'fnlwgt', 'education-num'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_emb_sz(to, sz_dict=None):\n",
    "    \"Get default embedding size from `TabularPreprocessor` `proc` or the ones in `sz_dict`\"\n",
    "    return [_one_emb_sz(to.classes, n, sz_dict) for n in to.cat_names]\n",
    "\n",
    "def _one_emb_sz(classes, n, sz_dict=None):\n",
    "    \"Pick an embedding size for `n` depending on `classes` if not given in `sz_dict`.\"\n",
    "    sz_dict = ifnone(sz_dict, {})\n",
    "    n_cat = len(classes[n])\n",
    "    sz = sz_dict.get(n, int(emb_sz_rule(n_cat)))  # rule of thumb\n",
    "    return n_cat,sz\n",
    "\n",
    "def emb_sz_rule(n_cat):\n",
    "    \"Rule of thumb to pick embedding size corresponding to `n_cat`\"\n",
    "    return min(600, round(1.6 * n_cat**0.56))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "np.random.seed(41)\n",
    "splits = RandomSplitter()(range_of(df))\n",
    "\n",
    "cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\n",
    "cont_names = ['age', 'fnlwgt', 'education-num']\n",
    "procs = [Categorify, FillMissing, Normalize]\n",
    "y_names = 'salary'\n",
    "y_block = CategoryBlock()\n",
    "\n",
    "to = TabularPandas(df, procs=procs, cat_names=cat_names, cont_names=cont_names,\n",
    "                   y_names=y_names, y_block=y_block, splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "dls = to.dataloaders(bs=4096)\n",
    "emb_szs = get_emb_sz(to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "\n",
    "### Main components of the paper\n",
    "\n",
    "  - TabNetEncoder\n",
    "    - Feature Transformer\n",
    "      - Primitive Block\n",
    "        - FC + Ghost BN + GLU\n",
    "    - Attention Transformer\n",
    "      - FC, BN, Priors and SparseMax\n",
    "    - Split\n",
    "\n",
    "  - TabNetDecoder\n",
    "\n",
    "### How to create a primitive block?\n",
    "  - Primitive Block consists of FC ( fully-connected ) layer, ghost batch norm and GLU ( Gated Linear Unit ).\n",
    "  - According to design mentioned in the paper we have to support construction of multiple of these blocks.\n",
    "\n",
    "### What is the notion of decision step?\n",
    "  - Decision step is when we run tabnet encoder multiple times.\n",
    "\n",
    "### What is a shared decision unit?\n",
    "  - During multiple runs of the tabnet encoder the section of the feature transformer that would share weights across the runs.\n",
    "\n",
    "### What is an independent decision unit?\n",
    "  - During multiple runs of the tabnet encoder the section of the feature transformer that would not share weights across the runs.\n",
    "\n",
    "### Which components of the primitive blocks are parameterized?\n",
    "  - FC\n",
    "  - Ghost BN ( batch_size, momentum )\n",
    "\n",
    "### What is meant by muliple copies of the primitive block?\n",
    "  - So the paper proposes to try out building multiple different copies of the primitive block, every copy would require us to initialize different parameters which could or could not be shared across different decision steps.\n",
    "\n",
    "### Notes:\n",
    "  - In this architecture we are assuming that shared layer would always come before the independent decision step layer, but what would happend there is no shared layer and we only want independent decision layer.\n",
    "  - Also in feature transformer, Linear layer takes input_dim to be #features and output dimension would be 2 * (n_d + n_a) where n_d represents `dimension of the prediction layer` and n_a represents `dimension of the attention layer`.\n",
    "\n",
    "### Todos:\n",
    "  - [x] initialize glu layer particular linear layer with xavier initializaion, look at fastai docs to see how linear layer is initialized in the module.\n",
    "  - [x] Attention Module\n",
    "  - [x] Split Module\n",
    "  - [x] Loss function\n",
    "  - [x] Introduce $\\lambda_{sparse}$ and add it to the overall loss.\n",
    "  - [ ] TabNetDecoder ( for self-supervised learning )\n",
    "  - [ ] Explainable AI through global and instance level feature importance.\n",
    "  - [ ] How to fit decoder network in the cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def init_glu_linear(module, input_dim, output_dim):\n",
    "    gain_value = np.sqrt((input_dim + output_dim) / np.sqrt(4 * input_dim))\n",
    "    nn.init.xavier_normal_(module.weight, gain=gain_value)\n",
    "\n",
    "def init_non_glu_linear(module, input_dim, output_dim):\n",
    "    gain_value = np.sqrt((input_dim + output_dim) / np.sqrt(input_dim))\n",
    "    torch.nn.init.xavier_normal_(module.weight, gain=gain_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainable Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TNEMbedding(Module):\n",
    "  \"Embedding layer used in Tab Net\"\n",
    "  def __init__(self, emb_szs, n_cont):\n",
    "      self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])\n",
    "      n_emb = sum(e.embedding_dim for e in self.embeds)\n",
    "      self.n_emb,self.n_cont = n_emb,n_cont\n",
    "      sizes = [n_emb + n_cont]\n",
    "      \n",
    "  def forward(self, x_cat, x_cont=None):\n",
    "      \"\"\"\n",
    "      Runs through categorical features and transform them into\n",
    "      embeding based on the `emb_szs` passed in the layer constructor.\n",
    "      It then concatenates the remaining continuous features if any with\n",
    "      the embeddings.\n",
    "      \"\"\"\n",
    "\n",
    "      if self.n_emb != 0:\n",
    "          x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n",
    "          x = torch.cat(x, 1)\n",
    "      \n",
    "      if self.n_cont != 0:\n",
    "          x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n",
    "      \n",
    "      return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ghost Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GBN(Module):\n",
    "    \"\"\"\n",
    "    Ghost Batch Normalization\n",
    "    https://arxiv.org/abs/1705.08741\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inp_dim, vbs=128, mom=0.01):\n",
    "        store_attr()\n",
    "        self.bn = nn.BatchNorm1d(self.inp_dim, momentum=self.mom)\n",
    "\n",
    "    def forward(self, x):\n",
    "        chunks = x.chunk(int(np.ceil(x.shape[0] / self.vbs)), 0)\n",
    "        res    = [self.bn(x_) for x_ in chunks]\n",
    "        return torch.cat(res, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLU Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GLUBlock(Module):\n",
    "  def __init__(self, inp_dim, out_dim, fc=None, vbs=128, mom=0.02):\n",
    "    store_attr()\n",
    "\n",
    "    self.fc = ifnone(fc, nn.Linear(inp_dim, 2 * out_dim, bias=False))\n",
    "    init_glu_linear(self.fc, inp_dim, 2 * out_dim)\n",
    "    \n",
    "    self.bn = GBN(2 * out_dim, vbs=vbs, mom=mom)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.fc(x)\n",
    "    out = self.bn(out)\n",
    "    out = torch.mul(out[:, : self.out_dim], torch.sigmoid(out[:, self.out_dim :]))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FeatureTransformer(Module):\n",
    "  def __init__(self, inp_dim, out_dim, n_d, n_a, n_sh, n_ind, vbs, mom):\n",
    "    store_attr()\n",
    "\n",
    "    self.fcs    = self._make_fc(n_sh)\n",
    "    self.sh_ds  = self._make_shared_blocks()\n",
    "    self.ind_ds = self._make_ind_blocks() \n",
    "\n",
    "  \n",
    "  def _make_fc(self, n):\n",
    "    return nn.ModuleList([nn.Linear(self.inp_dim, \n",
    "                                    2 * (self.n_d + self.n_a), \n",
    "                                    bias=False) if i == 0 else \\\n",
    "                          nn.Linear(self.n_d + self.n_a, \n",
    "                                    2 * (self.n_d + self.n_a),\n",
    "                                    bias=False\n",
    "                                    ) for i in range(n)])\n",
    "\n",
    "  def _make_shared_blocks(self):\n",
    "    return nn.ModuleList([GLUBlock(self.inp_dim, \n",
    "                                    self.out_dim, \n",
    "                                    fc=self.fcs[i], \n",
    "                                    vbs=self.vbs, \n",
    "                                    mom=self.mom) if i == 0 else\\\n",
    "                           GLUBlock(self.out_dim, \n",
    "                                    self.out_dim, \n",
    "                                    fc=self.fcs[i], \n",
    "                                    vbs=self.vbs, \n",
    "                                    mom=self.mom) for i in range(self.n_sh)])\n",
    "    \n",
    "  def _make_ind_blocks(self):\n",
    "    return nn.ModuleList([GLUBlock(self.out_dim, \n",
    "                                   self.out_dim, \n",
    "                                   fc=None, \n",
    "                                   vbs=self.vbs, \n",
    "                                   mom=self.mom) for i in range(self.n_ind)]\n",
    "                         )\n",
    "\n",
    "  def forward(self, x):\n",
    "    scale = torch.sqrt(torch.FloatTensor([0.5])).to(x.device)\n",
    "\n",
    "    out = self.sh_ds[0](x)\n",
    "    \n",
    "    for i in range(1, len(self.sh_ds)):\n",
    "      out = torch.add(out, self.sh_ds[i](out))\n",
    "      out = out * scale\n",
    "\n",
    "    for i in range(len(self.ind_ds)):\n",
    "      out = torch.add(out, self.ind_ds[i](out))\n",
    "      out = out * scale\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparsemax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# credits to Yandex https://github.com/Qwicen/node/blob/master/lib/nn_utils.py\n",
    "def _make_ix_like(input, dim=0):\n",
    "    d = input.size(dim)\n",
    "    rho = torch.arange(1, d + 1, device=input.device, dtype=input.dtype)\n",
    "    view = [1] * input.dim()\n",
    "    view[0] = -1\n",
    "    return rho.view(view).transpose(0, dim)\n",
    "\n",
    "\n",
    "class SparsemaxFunction(Function):\n",
    "    \"\"\"\n",
    "    An implementation of sparsemax (Martins & Astudillo, 2016). See\n",
    "    :cite:`DBLP:journals/corr/MartinsA16` for detailed description.\n",
    "    By Ben Peters and Vlad Niculae\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, dim=-1):\n",
    "        \"\"\"sparsemax: normalizing sparse transform (a la softmax)\n",
    "        Parameters\n",
    "        ----------\n",
    "        ctx : torch.autograd.function._ContextMethodMixin\n",
    "        input : torch.Tensor\n",
    "            any shape\n",
    "        dim : int\n",
    "            dimension along which to apply sparsemax\n",
    "        Returns\n",
    "        -------\n",
    "        output : torch.Tensor\n",
    "            same shape as input\n",
    "        \"\"\"\n",
    "        ctx.dim = dim\n",
    "        max_val, _ = input.max(dim=dim, keepdim=True)\n",
    "        input -= max_val  # same numerical stability trick as for softmax\n",
    "        tau, supp_size = SparsemaxFunction._threshold_and_support(input, dim=dim)\n",
    "        output = torch.clamp(input - tau, min=0)\n",
    "        ctx.save_for_backward(supp_size, output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        supp_size, output = ctx.saved_tensors\n",
    "        dim = ctx.dim\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[output == 0] = 0\n",
    "\n",
    "        v_hat = grad_input.sum(dim=dim) / supp_size.to(output.dtype).squeeze()\n",
    "        v_hat = v_hat.unsqueeze(dim)\n",
    "        grad_input = torch.where(output != 0, grad_input - v_hat, grad_input)\n",
    "        return grad_input, None\n",
    "\n",
    "    @staticmethod\n",
    "    def _threshold_and_support(input, dim=-1):\n",
    "        \"\"\"Sparsemax building block: compute the threshold\n",
    "        Parameters\n",
    "        ----------\n",
    "        input: torch.Tensor\n",
    "            any dimension\n",
    "        dim : int\n",
    "            dimension along which to apply the sparsemax\n",
    "        Returns\n",
    "        -------\n",
    "        tau : torch.Tensor\n",
    "            the threshold value\n",
    "        support_size : torch.Tensor\n",
    "        \"\"\"\n",
    "\n",
    "        input_srt, _ = torch.sort(input, descending=True, dim=dim)\n",
    "        input_cumsum = input_srt.cumsum(dim) - 1\n",
    "        rhos = _make_ix_like(input, dim)\n",
    "        support = rhos * input_srt > input_cumsum\n",
    "\n",
    "        support_size = support.sum(dim=dim).unsqueeze(dim)\n",
    "        tau = input_cumsum.gather(dim, support_size - 1)\n",
    "        tau /= support_size.to(input.dtype)\n",
    "        return tau, support_size\n",
    "\n",
    "\n",
    "sparsemax = SparsemaxFunction.apply\n",
    "\n",
    "class Sparsemax(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        self.dim = dim\n",
    "        super(Sparsemax, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return sparsemax(input, self.dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attentive Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AttentiveTransformer(Module):\n",
    "  def __init__(self, inp_dim, out_dim, vbs, mom, mask_type):\n",
    "    store_attr()\n",
    "    self.fc = nn.Linear(inp_dim, out_dim, bias=False)\n",
    "    init_non_glu_linear(self.fc, inp_dim, out_dim)\n",
    "    \n",
    "    self.bn = GBN(out_dim, vbs=vbs, mom=mom)\n",
    "\n",
    "    mask_type = ifnone(mask_type, 'sparsemax')\n",
    "    if mask_type == 'sparsemax': self.sel = Sparsemax(dim=-1)\n",
    "\n",
    "  def forward(self, priors, proc_feat):\n",
    "    out = self.fc(proc_feat)\n",
    "    out = self.bn(out)\n",
    "    out = torch.mul(out, priors)\n",
    "    out = self.sel(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TabNetEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TabNetEncoder(Module):\n",
    "  def __init__(self, \n",
    "               inp_dim, \n",
    "               out_dim,\n",
    "               n_d,\n",
    "               n_a,\n",
    "               n_ds,\n",
    "               gamma=1.3,\n",
    "               n_ind=2,\n",
    "               n_sh=2,\n",
    "               eps=1e-15,\n",
    "               vbs=128,\n",
    "               mom=0.02,\n",
    "               mask_type='sparsemax'\n",
    "               ):\n",
    "    \n",
    "    store_attr()\n",
    "    self.init_bn = nn.BatchNorm1d(self.inp_dim, momentum=0.01)\n",
    "    self.init_ft = FeatureTransformer(inp_dim,\n",
    "                                      n_d + n_a,\n",
    "                                      n_d,\n",
    "                                      n_a,\n",
    "                                      n_sh, \n",
    "                                      n_ind, \n",
    "                                      vbs, \n",
    "                                      mom\n",
    "                                      )\n",
    "    \n",
    "    # based on number of decision steps we would create module\n",
    "    # list of transformers and attentive transformers\n",
    "    self.fts = nn.ModuleList()\n",
    "    self.ats = nn.ModuleList()\n",
    "\n",
    "    for i in range(n_ds):\n",
    "      ft = FeatureTransformer(inp_dim,\n",
    "                              n_d + n_a,\n",
    "                              n_d,\n",
    "                              n_a,\n",
    "                              n_sh, \n",
    "                              n_ind, \n",
    "                              vbs, \n",
    "                              mom\n",
    "                              )\n",
    "      \n",
    "      # attentive transformer is always preceded by \n",
    "      # feature transformer hence inp_dim would be \n",
    "      # `n_d + n_a`\n",
    "      \n",
    "      at = AttentiveTransformer(n_a,\n",
    "                                inp_dim,\n",
    "                                vbs=vbs,\n",
    "                                mom=mom,\n",
    "                                mask_type=mask_type\n",
    "                                )\n",
    "      \n",
    "      self.fts.append(ft)\n",
    "      self.ats.append(at)\n",
    "\n",
    "  def forward(self, x, priors=None):\n",
    "    x = self.init_bn(x)\n",
    "\n",
    "    priors = ifnone(priors, torch.ones(x.shape).to(x.device))\n",
    "\n",
    "    M_loss = 0\n",
    "    att    = self.init_ft(x)[:, self.n_d:] # this could be configured\n",
    "\n",
    "    steps_out = []\n",
    "\n",
    "    for step in range(self.n_ds):\n",
    "      M = self.ats[step](priors, att)\n",
    "      M_loss += torch.mean(torch.sum(torch.mul(M, torch.log(M + self.eps)), dim=1)\n",
    "                          )\n",
    "      \n",
    "      # update prior\n",
    "      priors = torch.mul(self.gamma - priors, priors)\n",
    "\n",
    "      # output\n",
    "      masked_x = torch.mul(M, x)\n",
    "      out      = self.fts[step](masked_x)\n",
    "      d        = nn.ReLU()(out[:, :self.n_d])\n",
    "\n",
    "      steps_out.append(d)\n",
    "\n",
    "      # update attention\n",
    "      att = out[:, self.n_d:]\n",
    "\n",
    "    M_loss /= self.n_ds\n",
    "    return steps_out, M_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TabNet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TabNet(Module):\n",
    "  def __init__(self,\n",
    "               emb_szs, \n",
    "               n_cont,\n",
    "               out_dim,\n",
    "               n_d=2,\n",
    "               n_a=2,\n",
    "               n_ds=2,\n",
    "               gamma=1.3,\n",
    "               n_ind=2,\n",
    "               n_sh=2,\n",
    "               eps=1e-15,\n",
    "               vbs=128,\n",
    "               mom=0.02,\n",
    "               mask_type=\"sparsemax\",\n",
    "               ):\n",
    "    \n",
    "    store_attr()\n",
    "    self.tnembed = TNEMbedding(emb_szs, n_cont)\n",
    "    self.encoder = TabNetEncoder(inp_dim=self.tnembed.n_emb + n_cont, \n",
    "                                out_dim=out_dim,\n",
    "                                n_d=n_d,\n",
    "                                n_a=n_a,\n",
    "                                n_ds=n_ds,\n",
    "                                gamma=gamma,\n",
    "                                n_ind=n_ind,\n",
    "                                n_sh=n_sh,\n",
    "                                eps=eps,\n",
    "                                vbs=vbs,\n",
    "                                mom=mom,\n",
    "                                mask_type=mask_type\n",
    "                                )\n",
    "    \n",
    "    self.final_mapping = nn.Linear(n_d, out_dim, bias=False)\n",
    "    init_non_glu_linear(self.final_mapping, n_d, out_dim)\n",
    "\n",
    "  def forward(self, x_cat, x_cont):\n",
    "    x  = self.tnembed(x_cat, x_cont)\n",
    "\n",
    "    res = 0\n",
    "    steps_output, M_loss = self.encoder(x)\n",
    "    res = torch.sum(torch.stack(steps_output, dim=0), dim=0)\n",
    "    out = self.final_mapping(res)\n",
    "    \n",
    "    return out, M_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TabNetLoss(Module):\n",
    "  def __init__(self, lambda_sparse:float):\n",
    "    store_attr()\n",
    "    self.loss_fn = CrossEntropyLossFlat()\n",
    "  \n",
    "  def forward(self, os, target):\n",
    "      output, M_loss = os\n",
    "      tot            = self.loss_fn(output, target) +\\\n",
    "                       M_loss * self.lambda_sparse\n",
    "      return tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "model = TabNet(emb_szs=get_emb_sz(to),\n",
    "             n_cont=len(to.cont_names), \n",
    "             out_dim=dls.c, \n",
    "             n_d=16, \n",
    "             n_a=16,\n",
    "             n_ds=5, \n",
    "             n_sh=2,\n",
    "             n_ind=2, \n",
    "             gamma=1.5,\n",
    "             vbs=128, \n",
    "             mom=0.02\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def custom_accuracy(inps, targ, axis=-1):\n",
    "  \"Compute accuracy with `targ` when `pred` is bs * n_classes\"\n",
    "  inp, _ = inps\n",
    "  pred,targ = flatten_check(inp.argmax(dim=axis), targ)\n",
    "  return (pred == targ).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "lambda_sparse = 1e-4\n",
    "learn = Learner(dls, model, TabNetLoss(lambda_sparse), opt_func=ranger, metrics=[custom_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "lambda_sparse = 1e-4\n",
    "opt_func = partial(Adam, wd=0.01, eps=1e-5)\n",
    "learn = Learner(dls, model, TabNetLoss(lambda_sparse), opt_func=opt_func, lr=1e-2, metrics=[custom_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       ""
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(valley=0.013182567432522774)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>custom_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.945846</td>\n",
       "      <td>0.518069</td>\n",
       "      <td>0.769656</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.757278</td>\n",
       "      <td>0.549525</td>\n",
       "      <td>0.755221</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.675667</td>\n",
       "      <td>0.512002</td>\n",
       "      <td>0.765817</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.620661</td>\n",
       "      <td>0.497234</td>\n",
       "      <td>0.777027</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.581336</td>\n",
       "      <td>0.490098</td>\n",
       "      <td>0.775184</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.549457</td>\n",
       "      <td>0.481839</td>\n",
       "      <td>0.790694</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.523759</td>\n",
       "      <td>0.468211</td>\n",
       "      <td>0.794533</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.502621</td>\n",
       "      <td>0.461106</td>\n",
       "      <td>0.801904</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.485134</td>\n",
       "      <td>0.455618</td>\n",
       "      <td>0.800369</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.470272</td>\n",
       "      <td>0.444955</td>\n",
       "      <td>0.801136</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.457660</td>\n",
       "      <td>0.436848</td>\n",
       "      <td>0.799447</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.446366</td>\n",
       "      <td>0.430432</td>\n",
       "      <td>0.801290</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.436542</td>\n",
       "      <td>0.428286</td>\n",
       "      <td>0.799908</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.428122</td>\n",
       "      <td>0.427602</td>\n",
       "      <td>0.798219</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.420750</td>\n",
       "      <td>0.423940</td>\n",
       "      <td>0.800983</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.414040</td>\n",
       "      <td>0.423021</td>\n",
       "      <td>0.802518</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.408010</td>\n",
       "      <td>0.420445</td>\n",
       "      <td>0.803286</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.402581</td>\n",
       "      <td>0.421541</td>\n",
       "      <td>0.800676</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.397351</td>\n",
       "      <td>0.419753</td>\n",
       "      <td>0.802365</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.393173</td>\n",
       "      <td>0.418883</td>\n",
       "      <td>0.804054</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.389609</td>\n",
       "      <td>0.419437</td>\n",
       "      <td>0.800522</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.386249</td>\n",
       "      <td>0.418100</td>\n",
       "      <td>0.803593</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.383389</td>\n",
       "      <td>0.418514</td>\n",
       "      <td>0.804208</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.380792</td>\n",
       "      <td>0.416549</td>\n",
       "      <td>0.798679</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.377984</td>\n",
       "      <td>0.417020</td>\n",
       "      <td>0.800061</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.375718</td>\n",
       "      <td>0.416090</td>\n",
       "      <td>0.804054</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.373250</td>\n",
       "      <td>0.414654</td>\n",
       "      <td>0.802826</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.370962</td>\n",
       "      <td>0.414818</td>\n",
       "      <td>0.800983</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.368960</td>\n",
       "      <td>0.412577</td>\n",
       "      <td>0.808047</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.367206</td>\n",
       "      <td>0.411003</td>\n",
       "      <td>0.803900</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.365573</td>\n",
       "      <td>0.410784</td>\n",
       "      <td>0.808200</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.363942</td>\n",
       "      <td>0.407729</td>\n",
       "      <td>0.804975</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.362517</td>\n",
       "      <td>0.408224</td>\n",
       "      <td>0.806511</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.361036</td>\n",
       "      <td>0.407289</td>\n",
       "      <td>0.807740</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.359746</td>\n",
       "      <td>0.407625</td>\n",
       "      <td>0.809275</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.358593</td>\n",
       "      <td>0.405748</td>\n",
       "      <td>0.809122</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.357445</td>\n",
       "      <td>0.406779</td>\n",
       "      <td>0.810197</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.356096</td>\n",
       "      <td>0.403978</td>\n",
       "      <td>0.809275</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.354978</td>\n",
       "      <td>0.402582</td>\n",
       "      <td>0.810197</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.353964</td>\n",
       "      <td>0.401254</td>\n",
       "      <td>0.810964</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "learn.fit_flat_cos(40, lr=1e-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
