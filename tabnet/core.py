# AUTOGENERATED! DO NOT EDIT! File to edit: 00_core.ipynb (unless otherwise specified).

__all__ = ['get_emb_sz', 'emb_sz_rule', 'create_explain_matrix', 'make_dl', 'to', 'dls', 'emb_szs', 'init_glu_linear',
           'init_non_glu_linear', 'TNEMbedding', 'GBN', 'GLUBlock', 'FeatureTransformer', 'SparsemaxFunction',
           'Sparsemax', 'sparsemax', 'AttentiveTransformer', 'TabNetEncoder', 'TabNet', 'TabNetLoss']

# Cell
import scipy as sp
from fastai.tabular.all import *
from torch.autograd     import Function

# Cell
def get_emb_sz(to, sz_dict=None):
    "Get default embedding size from `TabularPreprocessor` `proc` or the ones in `sz_dict`"
    return [_one_emb_sz(to.classes, n, sz_dict) for n in to.cat_names]

def _one_emb_sz(classes, n, sz_dict=None):
    "Pick an embedding size for `n` depending on `classes` if not given in `sz_dict`."
    sz_dict = ifnone(sz_dict, {})
    n_cat = len(classes[n])
    sz = sz_dict.get(n, int(emb_sz_rule(n_cat)))  # rule of thumb
    return n_cat,sz

def emb_sz_rule(n_cat):
    "Rule of thumb to pick embedding size corresponding to `n_cat`"
    return min(600, round(1.6 * n_cat**0.56))

# Cell
def create_explain_matrix(input_dim, cat_emb_dim, cat_idxs, post_embed_dim):
    """
    This is a computational trick.
    In order to rapidly sum importances from same embeddings
    to the initial index.
    Parameters
    ----------
    input_dim : int
        Initial input dim
    cat_emb_dim : int or list of int
        if int : size of embedding for all categorical feature
        if list of int : size of embedding for each categorical feature
    cat_idxs : list of int
        Initial position of categorical features
    post_embed_dim : int
        Post embedding inputs dimension
    Returns
    -------
    reducing_matrix : np.array
        Matrix of dim (post_embed_dim, input_dim)  to performe reduce
    """

    if isinstance(cat_emb_dim, int):
        all_emb_impact = [cat_emb_dim - 1] * len(cat_idxs)
    else:
        all_emb_impact = [emb_dim - 1 for emb_dim in cat_emb_dim]

    acc_emb = 0
    nb_emb = 0
    indices_trick = []
    for i in range(input_dim):
        if i not in cat_idxs:
            indices_trick.append([i + acc_emb])
        else:
            indices_trick.append(
                range(i + acc_emb, i + acc_emb + all_emb_impact[nb_emb] + 1)
            )
            acc_emb += all_emb_impact[nb_emb]
            nb_emb += 1

    reducing_matrix = np.zeros((post_embed_dim, input_dim))
    for i, cols in enumerate(indices_trick):
        reducing_matrix[cols, i] = 1

    return sp.sparse.csc_matrix(reducing_matrix)

# Cell
def make_dl():
  path = untar_data(URLs.ADULT_SAMPLE)
  df = pd.read_csv(path / 'adult.csv')
  cat_names  = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']
  cont_names = ['age', 'fnlwgt', 'education-num']

  np.random.seed(41)
  splits = RandomSplitter()(range_of(df))

  cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']
  cont_names = ['age', 'fnlwgt', 'education-num']
  procs = [Categorify, FillMissing, Normalize]
  y_names = 'salary'
  y_block = CategoryBlock()

  to = TabularPandas(df, procs=procs, cat_names=cat_names, cont_names=cont_names,
                    y_names=y_names, y_block=y_block, splits=splits)

  return to

# Cell
to  = make_dl()
dls = to.dataloaders(bs=4096)
emb_szs = get_emb_sz(to)

# Cell
def init_glu_linear(module, input_dim, output_dim):
    gain_value = np.sqrt((input_dim + output_dim) / np.sqrt(4 * input_dim))
    nn.init.xavier_normal_(module.weight, gain=gain_value)

def init_non_glu_linear(module, input_dim, output_dim):
    gain_value = np.sqrt((input_dim + output_dim) / np.sqrt(input_dim))
    torch.nn.init.xavier_normal_(module.weight, gain=gain_value)

# Cell
class TNEMbedding(Module):
  "Embedding layer used in Tab Net"
  def __init__(self, emb_szs, n_cont):
      self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])
      n_emb = sum(e.embedding_dim for e in self.embeds)
      self.n_emb,self.n_cont = n_emb,n_cont
      sizes = [n_emb + n_cont]

  def forward(self, x_cat, x_cont=None):
      """
      Runs through categorical features and transform them into
      embeding based on the `emb_szs` passed in the layer constructor.
      It then concatenates the remaining continuous features if any with
      the embeddings.
      """

      if self.n_emb != 0:
          x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]
          x = torch.cat(x, 1)

      if self.n_cont != 0:
          x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont

      return x

# Cell
class GBN(Module):
    """
    Ghost Batch Normalization
    https://arxiv.org/abs/1705.08741
    """

    def __init__(self, inp_dim, vbs=128, mom=0.01):
        store_attr()
        self.bn = nn.BatchNorm1d(self.inp_dim, momentum=self.mom)

    def forward(self, x):
        chunks = x.chunk(int(np.ceil(x.shape[0] / self.vbs)), 0)
        res    = [self.bn(x_) for x_ in chunks]
        return torch.cat(res, dim=0)

# Cell
class GLUBlock(Module):
  def __init__(self, inp_dim, out_dim, fc=None, vbs=128, mom=0.02):
    store_attr()

    self.fc = ifnone(fc, nn.Linear(inp_dim, 2 * out_dim, bias=False))
    init_glu_linear(self.fc, inp_dim, 2 * out_dim)

    self.bn = GBN(2 * out_dim, vbs=vbs, mom=mom)

  def forward(self, x):
    out = self.fc(x)
    out = self.bn(out)
    out = torch.mul(out[:, : self.out_dim], torch.sigmoid(out[:, self.out_dim :]))
    return out

# Cell
class FeatureTransformer(Module):
  def __init__(self, inp_dim, out_dim, n_d, n_a, n_sh, n_ind, vbs, mom):
    store_attr()

    self.fcs    = self._make_fc(n_sh)
    self.sh_ds  = self._make_shared_blocks()
    self.ind_ds = self._make_ind_blocks()


  def _make_fc(self, n):
    return nn.ModuleList([nn.Linear(self.inp_dim,
                                    2 * (self.n_d + self.n_a),
                                    bias=False) if i == 0 else \
                          nn.Linear(self.n_d + self.n_a,
                                    2 * (self.n_d + self.n_a),
                                    bias=False
                                    ) for i in range(n)])

  def _make_shared_blocks(self):
    return nn.ModuleList([GLUBlock(self.inp_dim,
                                    self.out_dim,
                                    fc=self.fcs[i],
                                    vbs=self.vbs,
                                    mom=self.mom) if i == 0 else\
                           GLUBlock(self.out_dim,
                                    self.out_dim,
                                    fc=self.fcs[i],
                                    vbs=self.vbs,
                                    mom=self.mom) for i in range(self.n_sh)])

  def _make_ind_blocks(self):
    return nn.ModuleList([GLUBlock(self.out_dim,
                                   self.out_dim,
                                   fc=None,
                                   vbs=self.vbs,
                                   mom=self.mom) for i in range(self.n_ind)]
                         )

  def forward(self, x):
    scale = torch.sqrt(torch.FloatTensor([0.5])).to(x.device)

    out = self.sh_ds[0](x)

    for i in range(1, len(self.sh_ds)):
      out = torch.add(out, self.sh_ds[i](out))
      out = out * scale

    for i in range(len(self.ind_ds)):
      out = torch.add(out, self.ind_ds[i](out))
      out = out * scale

    return out

# Cell
# credits to Yandex https://github.com/Qwicen/node/blob/master/lib/nn_utils.py
def _make_ix_like(input, dim=0):
    d = input.size(dim)
    rho = torch.arange(1, d + 1, device=input.device, dtype=input.dtype)
    view = [1] * input.dim()
    view[0] = -1
    return rho.view(view).transpose(0, dim)


class SparsemaxFunction(Function):
    """
    An implementation of sparsemax (Martins & Astudillo, 2016). See
    :cite:`DBLP:journals/corr/MartinsA16` for detailed description.
    By Ben Peters and Vlad Niculae
    """

    @staticmethod
    def forward(ctx, input, dim=-1):
        """sparsemax: normalizing sparse transform (a la softmax)
        Parameters
        ----------
        ctx : torch.autograd.function._ContextMethodMixin
        input : torch.Tensor
            any shape
        dim : int
            dimension along which to apply sparsemax
        Returns
        -------
        output : torch.Tensor
            same shape as input
        """
        ctx.dim = dim
        max_val, _ = input.max(dim=dim, keepdim=True)
        input -= max_val  # same numerical stability trick as for softmax
        tau, supp_size = SparsemaxFunction._threshold_and_support(input, dim=dim)
        output = torch.clamp(input - tau, min=0)
        ctx.save_for_backward(supp_size, output)
        return output

    @staticmethod
    def backward(ctx, grad_output):
        supp_size, output = ctx.saved_tensors
        dim = ctx.dim
        grad_input = grad_output.clone()
        grad_input[output == 0] = 0

        v_hat = grad_input.sum(dim=dim) / supp_size.to(output.dtype).squeeze()
        v_hat = v_hat.unsqueeze(dim)
        grad_input = torch.where(output != 0, grad_input - v_hat, grad_input)
        return grad_input, None

    @staticmethod
    def _threshold_and_support(input, dim=-1):
        """Sparsemax building block: compute the threshold
        Parameters
        ----------
        input: torch.Tensor
            any dimension
        dim : int
            dimension along which to apply the sparsemax
        Returns
        -------
        tau : torch.Tensor
            the threshold value
        support_size : torch.Tensor
        """

        input_srt, _ = torch.sort(input, descending=True, dim=dim)
        input_cumsum = input_srt.cumsum(dim) - 1
        rhos = _make_ix_like(input, dim)
        support = rhos * input_srt > input_cumsum

        support_size = support.sum(dim=dim).unsqueeze(dim)
        tau = input_cumsum.gather(dim, support_size - 1)
        tau /= support_size.to(input.dtype)
        return tau, support_size


sparsemax = SparsemaxFunction.apply

class Sparsemax(nn.Module):
    def __init__(self, dim=-1):
        self.dim = dim
        super(Sparsemax, self).__init__()

    def forward(self, input):
        return sparsemax(input, self.dim)

# Cell
class AttentiveTransformer(Module):
  def __init__(self, inp_dim, out_dim, vbs, mom, mask_type):
    store_attr()
    self.fc = nn.Linear(inp_dim, out_dim, bias=False)
    init_non_glu_linear(self.fc, inp_dim, out_dim)

    self.bn = GBN(out_dim, vbs=vbs, mom=mom)

    mask_type = ifnone(mask_type, 'sparsemax')
    if mask_type == 'sparsemax': self.sel = Sparsemax(dim=-1)

  def forward(self, priors, proc_feat):
    out = self.fc(proc_feat)
    out = self.bn(out)
    out = torch.mul(out, priors)
    out = self.sel(out)
    return out

# Cell
class TabNetEncoder(Module):
  def __init__(self,
               inp_dim,
               out_dim,
               n_d,
               n_a,
               n_ds,
               gamma=1.3,
               n_ind=2,
               n_sh=2,
               eps=1e-15,
               vbs=128,
               mom=0.02,
               mask_type='sparsemax'
               ):

    store_attr()
    self.init_bn = nn.BatchNorm1d(self.inp_dim, momentum=0.01)
    self.init_ft = FeatureTransformer(inp_dim,
                                      n_d + n_a,
                                      n_d,
                                      n_a,
                                      n_sh,
                                      n_ind,
                                      vbs,
                                      mom
                                      )

    # based on number of decision steps we would create module
    # list of transformers and attentive transformers
    self.fts = nn.ModuleList()
    self.ats = nn.ModuleList()

    for i in range(n_ds):
      ft = FeatureTransformer(inp_dim,
                              n_d + n_a,
                              n_d,
                              n_a,
                              n_sh,
                              n_ind,
                              vbs,
                              mom
                              )

      # attentive transformer is always preceded by
      # feature transformer hence inp_dim would be
      # `n_d + n_a`

      at = AttentiveTransformer(n_a,
                                inp_dim,
                                vbs=vbs,
                                mom=mom,
                                mask_type=mask_type
                                )

      self.fts.append(ft)
      self.ats.append(at)

  def forward(self, x, priors=None):
    x = self.init_bn(x)

    priors = ifnone(priors, torch.ones(x.shape).to(x.device))

    M_loss = 0
    att    = self.init_ft(x)[:, self.n_d:] # this could be configured

    steps_out = []

    for step in range(self.n_ds):
      M = self.ats[step](priors, att)
      M_loss += torch.mean(torch.sum(torch.mul(M, torch.log(M + self.eps)), dim=1)
                          )

      # update prior
      priors = torch.mul(self.gamma - priors, priors)

      # output
      masked_x = torch.mul(M, x)
      out      = self.fts[step](masked_x)
      d        = nn.ReLU()(out[:, :self.n_d])

      steps_out.append(d)

      # update attention
      att = out[:, self.n_d:]

    M_loss /= self.n_ds
    return steps_out, M_loss

  def forward_masks(self, x):
    x = self.init_bn(x)
    priors = torch.ones(x.shape).to(x.device)
    M_explain = torch.zeros(x.shape).to(x.device)
    att = self.init_ft(x)[:, self.n_d:]
    masks = {}

    for step in range(self.n_ds):
      M = self.ats[step](priors, att)
      masks[step] = M

      # update priors
      priors = torch.mul(self.gamma - priors, priors)
      # output
      masked_x = torch.mul(M, x)
      out = self.fts[step](masked_x)
      d   = nn.ReLU()(out[:, :self.n_d])

      # explain aggregation
      step_importance = torch.sum(d, axis=1)
      M_explain += torch.mul(M, step_importance.unsqueeze(dim=1))

      # update attention
      att = out[:, self.n_d:]

    return M_explain, masks

# Cell
class TabNet(Module):
  def __init__(self,
               emb_szs,
               n_cont,
               out_dim,
               n_d=2,
               n_a=2,
               n_ds=2,
               gamma=1.3,
               n_ind=2,
               n_sh=2,
               eps=1e-15,
               vbs=128,
               mom=0.02,
               mask_type="sparsemax",
               ):

    store_attr()
    self.tnembed = TNEMbedding(emb_szs, n_cont)
    self.encoder = TabNetEncoder(inp_dim=self.tnembed.n_emb + n_cont,
                                out_dim=out_dim,
                                n_d=n_d,
                                n_a=n_a,
                                n_ds=n_ds,
                                gamma=gamma,
                                n_ind=n_ind,
                                n_sh=n_sh,
                                eps=eps,
                                vbs=vbs,
                                mom=mom,
                                mask_type=mask_type
                                )

    self.final_mapping = nn.Linear(n_d, out_dim, bias=False)
    init_non_glu_linear(self.final_mapping, n_d, out_dim)

  def forward(self, x_cat, x_cont, att=False):
    x  = self.tnembed(x_cat, x_cont)

    res = 0
    steps_output, M_loss = self.encoder(x)
    res = torch.sum(torch.stack(steps_output, dim=0), dim=0)
    out = self.final_mapping(res)

    if att:
      M_explain, masks = self.encoder.forward_masks(x)
      return out, M_loss, M_explain, masks

    return out, M_loss

# Cell
class TabNetLoss(Module):
  def __init__(self, lambda_sparse:float, axis:int=-1):
    store_attr()
    self.loss_fn = CrossEntropyLossFlat()

  def forward(self, os, target):
      output, M_loss = os
      tot            = self.loss_fn(output, target) +\
                       M_loss * self.lambda_sparse
      return tot
  def decodes(self, x):    return x.argmax(dim=self.axis)